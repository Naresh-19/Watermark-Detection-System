{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26788199",
   "metadata": {},
   "source": [
    "## **Watermark Detection in Images**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165df2a5",
   "metadata": {},
   "source": [
    "# Phase 0: Setup\n",
    "\n",
    "The setup phase initializes the project path and prepares to access the dataset.\n",
    "\n",
    "- Uses `os` and `pathlib.Path` to list all file paths under the raw dataset folder:  \n",
    "  `dataset/wm-nowm/`.\n",
    "- Counts and prints the total number of images available for processing.\n",
    "\n",
    "- Also Change the base_path directory based on your system path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac6fcfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files found: 31576\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Phase 0: Setup\n",
    "import os\n",
    "from PIL import Image  # Lightweight image processing\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the target directory\n",
    "base_path = Path(r\"D:\\project\\Watermark_Detection\\dataset\")\n",
    "\n",
    "# Phase 1: List all files with full path\n",
    "file_paths = []\n",
    "\n",
    "for root, dirs, files in os.walk(base_path / \"wm-nowm\"):\n",
    "    for file in files:\n",
    "        full_path = Path(root) / file\n",
    "        file_paths.append(full_path)\n",
    "\n",
    "\n",
    "print(f\"Total files found: {len(file_paths)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232cbc79",
   "metadata": {},
   "source": [
    "# Phase 1: Cleaning and Filtering Images\n",
    "\n",
    "This phase ensures data quality by selecting only valid image files.\n",
    "\n",
    "- Iterates through `watermark` and `no-watermark` folders under `wm-nowm/train/`.\n",
    "- Verifies each image using `PIL.Image.verify()` to check for corruption.\n",
    "- Copies the first 1000 valid images per category to `wm-nowm-cleaned/train/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3213f98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: watermark\n",
      "1000 valid images found for 'watermark'\n",
      "Processing category: no-watermark\n",
      "1000 valid images found for 'no-watermark'\n",
      "\n",
      "Cleaning & filtering completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import shutil\n",
    "\n",
    "source_root = base_path / \"wm-nowm\\train\"\n",
    "destination_root = base_path / \"wm-nowm-cleaned\\train\"\n",
    "\n",
    "categories = [\"watermark\", \"no-watermark\"]\n",
    "max_images_per_category = 1000\n",
    "\n",
    "for category in categories:\n",
    "    os.makedirs(destination_root / category, exist_ok=True)\n",
    "\n",
    "def is_valid_image(path):\n",
    "    try:\n",
    "        with Image.open(path) as img:\n",
    "            img.verify()  \n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "for category in categories:\n",
    "    print(f\"Processing category: {category}\")\n",
    "\n",
    "    source_folder = source_root / category\n",
    "    destination_folder = destination_root / category\n",
    "    \n",
    "    valid_images = []\n",
    "    \n",
    "    for img_file in source_folder.glob(\"**/*\"):\n",
    "        if img_file.suffix.lower() in [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".webp\"]:\n",
    "            if is_valid_image(img_file):\n",
    "                valid_images.append(img_file)\n",
    "        \n",
    "        if len(valid_images) >= max_images_per_category:\n",
    "            break\n",
    "\n",
    "    print(f\"{len(valid_images)} valid images found for '{category}'\")\n",
    "\n",
    "    for idx, valid_img in enumerate(valid_images):\n",
    "        target_path = destination_folder / f\"{category}_{idx}{valid_img.suffix}\"\n",
    "        shutil.copy2(valid_img, target_path)\n",
    "\n",
    "print(\"\\nCleaning & filtering completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757b7d3a",
   "metadata": {},
   "source": [
    "# Phase 2: Train/Validation Split\n",
    "\n",
    "Splits the cleaned dataset into `train` and `val` sets for both categories.\n",
    "\n",
    "- Randomly shuffles valid images.\n",
    "- Allocates 750 images per category to `train`, and 250 to `val`.\n",
    "- Saves the result under `wm-nowm-final/train/` and `wm-nowm-final/val/`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a6d5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: watermark\n",
      "Train images: 750 | Val images: 250\n",
      "Processing category: no-watermark\n",
      "Train images: 750 | Val images: 250\n",
      "\n",
      "Train/Val split completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "cleaned_root = base_path / \"wm-nowm-cleaned\\train\"\n",
    "\n",
    "final_dataset_root = base_path / \"wm-nowm-final\"\n",
    "\n",
    "categories = [\"watermark\", \"no-watermark\"]\n",
    "train_count = 750\n",
    "val_count = 250\n",
    "\n",
    "for split in ['train', 'val']:\n",
    "    for category in categories:\n",
    "        os.makedirs(final_dataset_root / split / category, exist_ok=True)\n",
    "\n",
    "for category in categories:\n",
    "    print(f\"Processing category: {category}\")\n",
    "    \n",
    "    all_images = list((cleaned_root / category).glob(\"*\"))\n",
    "    random.shuffle(all_images)\n",
    "\n",
    "    train_images = all_images[:train_count]\n",
    "    val_images = all_images[train_count:train_count + val_count]\n",
    "\n",
    "    print(f\"Train images: {len(train_images)} | Val images: {len(val_images)}\")\n",
    "\n",
    "    for img_path in train_images:\n",
    "        target_path = final_dataset_root / 'train' / category / img_path.name\n",
    "        shutil.copy2(img_path, target_path)\n",
    "\n",
    "    for img_path in val_images:\n",
    "        target_path = final_dataset_root / 'val' / category / img_path.name\n",
    "        shutil.copy2(img_path, target_path)\n",
    "\n",
    "print(\"\\nTrain/Val split completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e980326",
   "metadata": {},
   "source": [
    "# Phase 3: Device and Path Configuration\n",
    "\n",
    "Configures the environment and verifies dataset structure.\n",
    "\n",
    "- Detects if a GPU is available using `torch.cuda`.\n",
    "- Verifies presence of expected folders like `train/watermark`, `val/no-watermark`, etc.\n",
    "- Ensures the model does not fail due to missing directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255f2a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device selected: cuda\n",
      "Dataset folders verified.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device selected: {device}\")\n",
    "\n",
    "data_root = base_path / \"wm-nowm-final\"\n",
    "\n",
    "assert (data_root / 'train' / 'watermark').exists(), \"Train/Watermark folder missing!\"\n",
    "assert (data_root / 'train' / 'no-watermark').exists(), \"Train/No-Watermark folder missing!\"\n",
    "assert (data_root / 'val' / 'watermark').exists(), \"Val/Watermark folder missing!\"\n",
    "assert (data_root / 'val' / 'no-watermark').exists(), \"Val/No-Watermark folder missing!\"\n",
    "\n",
    "print(\"Dataset folders verified.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a184b9",
   "metadata": {},
   "source": [
    "# Phase 4: Data Transforms and Loaders\n",
    "\n",
    "Applies transforms and prepares data loaders.\n",
    "\n",
    "- Defines separate `transform_train` and `transform_val` using `torchvision.transforms`.\n",
    "- Normalizes images with ImageNet mean and standard deviation.\n",
    "- Uses `torchvision.datasets.ImageFolder` to load datasets.\n",
    "- Initializes PyTorch `DataLoader` objects for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff945b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 1915\n",
      "Validation dataset size: 500\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "input_size = 256\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_data = datasets.ImageFolder(root=r'D:\\project\\Watermark_Detection\\dataset\\wm-nowm-final\\train', transform=transform_train)\n",
    "val_data = datasets.ImageFolder(root=r'D:\\project\\Watermark_Detection\\dataset\\wm-nowm-final\\val', transform=transform_val)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dataloaders = {\n",
    "    'train': train_loader,\n",
    "    'val': val_loader\n",
    "}\n",
    "\n",
    "print(f\"Training dataset size: {len(train_data)}\")\n",
    "print(f\"Validation dataset size: {len(val_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82672488",
   "metadata": {},
   "source": [
    "# Phase 5: Model Definition - ResNeXt\n",
    "\n",
    "Initializes a `ResNeXt50_32x4d` model for binary classification.\n",
    "\n",
    "- Loads the pre-trained ResNeXt model.\n",
    "- Replaces the final classification layer to output 2 classes.\n",
    "- Moves the model to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121e8e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\HP-5C\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Users\\HP-5C\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V1`. You can also use `weights=ResNeXt50_32X4D_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "model_resnext = models.resnext50_32x4d(pretrained=True)\n",
    "\n",
    "num_ftrs_resnext = model_resnext.fc.in_features\n",
    "model_resnext.fc = nn.Linear(num_ftrs_resnext, 2) \n",
    "\n",
    "model_resnext = model_resnext.to('cuda')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb021e6",
   "metadata": {},
   "source": [
    "# Phase 6: Model Definition - ConvNeXt\n",
    "\n",
    "Initializes a `ConvNeXt Tiny` model using the `timm` library.\n",
    "\n",
    "- Loads a pre-trained ConvNeXt model via `timm.create_model`.\n",
    "- Sets `num_classes=2` to match the classification task.\n",
    "- Moves the model to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55a478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "model_convnext = timm.create_model('convnext_tiny', pretrained=True, num_classes=2)\n",
    "\n",
    "model_convnext = model_convnext.to('cuda')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a08939",
   "metadata": {},
   "source": [
    "# Phase 7: Model Definition - EfficientNet\n",
    "\n",
    "Initializes an `EfficientNet B3a` model using the `timm` library.\n",
    "\n",
    "- Loads a pre-trained EfficientNet model.\n",
    "- Configured for binary classification (`num_classes=2`).\n",
    "- Moves the model to GPU.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4500696d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\HP-5C\\anaconda3\\Lib\\site-packages\\timm\\models\\_factory.py:126: UserWarning: Mapping deprecated model name efficientnet_b3a to current efficientnet_b3.\n",
      "  model = create_fn(\n"
     ]
    }
   ],
   "source": [
    "model_effnet = timm.create_model('efficientnet_b3a', pretrained=True, num_classes=2)\n",
    "\n",
    "model_effnet = model_effnet.to('cuda')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0666be24",
   "metadata": {},
   "source": [
    "# Phase 8: Loss Function and Optimizers\n",
    "\n",
    "Defines the loss function and optimizers for each model.\n",
    "\n",
    "- Uses `CrossEntropyLoss` as the loss criterion.\n",
    "- Applies `AdamW` optimizer with a learning rate of `1e-4` for:\n",
    "  - ResNeXt\n",
    "  - ConvNeXt\n",
    "  - EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbfee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_resnext = optim.AdamW(model_resnext.parameters(), lr=1e-4)\n",
    "optimizer_convnext = optim.AdamW(model_convnext.parameters(), lr=1e-4)\n",
    "optimizer_effnet = optim.AdamW(model_effnet.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13481e9",
   "metadata": {},
   "source": [
    "# Phase 9: Training Loop\n",
    "\n",
    "Defines a reusable function `train_model()` to train any of the defined models.\n",
    "\n",
    "- Trains for a given number of epochs (default: 3).\n",
    "- Tracks loss and accuracy for both train and val sets.\n",
    "- Saves the best-performing model weights during validation.\n",
    "- Prints epoch-wise and final results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7117f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=3):\n",
    "    since = time.time()\n",
    "    val_acc_history = []\n",
    "    train_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train() \n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                inputs = inputs.to('cuda')\n",
    "                labels = labels.to('cuda')\n",
    "\n",
    "                optimizer.zero_grad()  \n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()  \n",
    "                        optimizer.step() \n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "            if phase == 'train':\n",
    "                train_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60}m {time_elapsed % 60}s')\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, train_acc_history, val_acc_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738574e3",
   "metadata": {},
   "source": [
    "# Phase 10: Train Model\n",
    "\n",
    "Trains the selected model(s) using the `train_model` function.\n",
    "\n",
    "- Currently, Only ConvNext models are trained , (Reason : `ConvNext Performed Well`)\n",
    "- Stores training and validation accuracy history for visualization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "949145cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ConvNeXt...\n",
      "Epoch 0/2\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [13:55<00:00, 13.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.4512 Acc: 0.7875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:15<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.4028 Acc: 0.8060\n",
      "\n",
      "Epoch 1/2\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [13:15<00:00, 13.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2542 Acc: 0.8935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:13<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2705 Acc: 0.9080\n",
      "\n",
      "Epoch 2/2\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [13:33<00:00, 13.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2177 Acc: 0.9076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:11<00:00,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2877 Acc: 0.8960\n",
      "\n",
      "Training complete in 41.0m 25.363177061080933s\n",
      "Best val Acc: 0.9080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "# # Train ResNeXt model\n",
    "# print(\"Training ResNeXt...\")\n",
    "# model_resnext, train_acc_history_resnext, val_acc_history_resnext = train_model(\n",
    "#     model_resnext, dataloaders, criterion, optimizer_resnext, num_epochs=3\n",
    "# )\n",
    "\n",
    "# Train ConvNeXt model\n",
    "print(\"Training ConvNeXt...\")\n",
    "model_convnext, train_acc_history_convnext, val_acc_history_convnext = train_model(\n",
    "    model_convnext, dataloaders, criterion, optimizer_convnext, num_epochs=3\n",
    ")\n",
    "\n",
    "# # Train EfficientNet model\n",
    "# print(\"Training EfficientNet...\")\n",
    "# model_effnet, train_acc_history_effnet, val_acc_history_effnet = train_model(\n",
    "#     model_effnet, dataloaders, criterion, optimizer_effnet, num_epochs=3\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a116d74",
   "metadata": {},
   "source": [
    "# Phase 11: Model Saving\n",
    "\n",
    "Saves the trained model weights to disk.\n",
    "\n",
    "- Saves ConvNeXt weights as `logoconvnext_best_model.pth`.\n",
    "- Includes optional commented lines to save ResNeXt and EfficientNet models as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787ed43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Save the best model weights\n",
    "# torch.save(model_resnext.state_dict(), 'resnext_best_model.pth')\n",
    "torch.save(model_convnext.state_dict(), 'logoconvnext_best_model.pth')\n",
    "# torch.save(model_effnet.state_dict(), 'effnet_best_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59fbde04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset restructuring completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Paths\n",
    "src_root = base_path / \"wm-nowm\"\n",
    "dst_root = base_path / \"wm-nowm-final\"\n",
    "\n",
    "# Make sure destination has train/val/test folders\n",
    "for split in ['train', 'val', 'test']:\n",
    "    for class_name in os.listdir(os.path.join(src_root, 'train')):\n",
    "        os.makedirs(os.path.join(dst_root, split, class_name), exist_ok=True)\n",
    "\n",
    "# Copy train and val fully first\n",
    "for split in ['train', 'val']:\n",
    "    src_split_path = os.path.join(src_root, split)\n",
    "    dst_split_path = os.path.join(dst_root, split)\n",
    "\n",
    "    for class_name in os.listdir(src_split_path):\n",
    "        src_class_path = os.path.join(src_split_path, class_name)\n",
    "        dst_class_path = os.path.join(dst_split_path, class_name)\n",
    "\n",
    "        for filename in os.listdir(src_class_path):\n",
    "            src_file = os.path.join(src_class_path, filename)\n",
    "            dst_file = os.path.join(dst_class_path, filename)\n",
    "            shutil.copy(src_file, dst_file)\n",
    "\n",
    "# Now randomly move few images from train/val to test\n",
    "num_test_images_per_class = 20  # you can change this number\n",
    "\n",
    "for split in ['train', 'val']:\n",
    "    split_path = os.path.join(dst_root, split)\n",
    "    for class_name in os.listdir(split_path):\n",
    "        class_path = os.path.join(split_path, class_name)\n",
    "        all_images = os.listdir(class_path)\n",
    "        random.shuffle(all_images)\n",
    "\n",
    "        test_images = all_images[:num_test_images_per_class]\n",
    "\n",
    "        for img in test_images:\n",
    "            src_img_path = os.path.join(class_path, img)\n",
    "            dst_img_path = os.path.join(dst_root, 'test', class_name, img)\n",
    "            shutil.move(src_img_path, dst_img_path)\n",
    "\n",
    "print(\"Dataset restructuring completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
